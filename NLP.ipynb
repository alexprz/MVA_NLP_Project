{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhwbv0PjoCw2yourH6eVl2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CpNMeoF-dSsm"},"source":["# Project for the course [Algorithms for speech and natural language processing](https://github.com/edupoux/MVA_2021_SL)\n","\n","Authors: Hugo Laurençon (hugo.laurencon@gmail.com), Alexandre Perez (alexandre.perez.enpc@gmail.com), Charbel-Raphaël Ségerie (charbel-raphael.segerie@hotmail.fr)\n","\n","Project proposal: https://fr.overleaf.com/project/601e7f56c6528f6574fe77e8"]},{"cell_type":"markdown","metadata":{"id":"vzwzdsPReUtG"},"source":["# Resources\n","\n","Unsupervised pretraining transfers well across languages: https://arxiv.org/abs/2002.02848\n","\n","Wav2Vec 2.0 Paper: https://arxiv.org/abs/2006.11477\n","\n","Wav2Vec2 Documentation: https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2forctc\n","\n","Other resources: https://docs.google.com/document/d/1P8pTAdIAZ14lZJzENwXBSFqUIUIdVa59SCTv1pXdhFs/edit#"]},{"cell_type":"markdown","metadata":{"id":"mWM3xNPRh5YP"},"source":["# Getting started"]},{"cell_type":"markdown","metadata":{"id":"QDJVP9ABgX-Z"},"source":["## Toy dataset of LibriSpeech"]},{"cell_type":"code","metadata":{"id":"oKb70U3uHrx-"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Jk1kxH3gcqo"},"source":["from datasets import load_dataset\n","import soundfile as sf\n","\n","def map_to_array(batch):\n","    speech, _ = sf.read(batch[\"file\"])\n","    batch[\"speech\"] = speech\n","    return batch\n","\n","ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n","ds = ds.map(map_to_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9r202nQ9hObK"},"source":["## Creation of a first pre-trained Wav2Vec model"]},{"cell_type":"code","metadata":{"id":"UJ04yDY5hLeG"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LT-3SjJjIKOl"},"source":["from transformers import Wav2Vec2Tokenizer, Wav2Vec2Model\n","\n","tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usbYQBGgihlV"},"source":["## Creation of embeddings for the toy dataset of LibriSpeech with the pre-trained Wav2Vec model"]},{"cell_type":"code","metadata":{"id":"Bt987nAFhYWJ"},"source":["input_values = tokenizer(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n","hidden_states = model(input_values).last_hidden_state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pghv_o0IWe0Z"},"source":["## Analysis\n","\n","ds est un dataset, comprenant entre autres des attributs 'text' et 'speech'.\n","\n","ds['text'] est une liste de phrase, qui sont les labels.\n","\n","ds['speech'] est une liste de liste. Chaque sous-liste a une taille variable selon la longueur de l'audio, mais est généralement d'une longueur au alentour de 100000 (notons qu'on ne considère qu'un signal mono et non stéréo), et comprend des nombres réels généralement compris entre -1 et 1 pour les valeurs les plus extrêmes.\n","\n","input_values = tokenizer(ds[\"speech\"][0], return_tensors=\"pt\").input_values renvoie essentiellement la même chose que ds[\"speech\"][0], mais en type tensor et avec un arrondi à la 4ème décimale après la virgule pour les valeurs de la liste.\n","\n","hidden_states = model(input_values).last_hidden_state renvoie un tensor de taille (1, N, 768), où 1 est ici le batch size, N dépend de la longueur de l'audio dans input_values (typiquement N est au alentour de 500), et 768 est la taille d'embedding."]},{"cell_type":"markdown","metadata":{"id":"rrAtKmoJOUee"},"source":["# Processing of the LibriSpeech dataset\n","\n","Download a dataset of your choice [here](https://www.openslr.org/12). The dev-clean dataset contains approximately 5 hours of read English speech.\n","\n","If using Google Colab, upload the dataset to your drive."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"op4NPrb1P6pI","executionInfo":{"status":"ok","timestamp":1616339268191,"user_tz":-60,"elapsed":16251,"user":{"displayName":"Hugo Laurençon","photoUrl":"","userId":"11375379862591739933"}},"outputId":"67692a70-1bfd-465e-df03-06409b8fe360"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OrAcaHmXi3Qi"},"source":["import os\n","\n","path_to_dataset = '/content/drive/My Drive/dev-clean'\n","\n","dic_flac_txt = {}\n","\n","for subdir, dirs, files in os.walk(path_to_dataset):\n","\n","    if len(files) > 1:\n","        filepath_txt = None\n","\n","        for file in files:\n","            if file.endswith(\".flac\"):\n","\n","                filepath_flac = subdir + os.sep + file\n","                data_flac, _ = sf.read(filepath_flac)\n","                dic_flac_txt[file[:-5]] = [data_flac, None]\n","\n","            elif file.endswith(\".txt\"):\n","                filepath_txt = subdir + os.sep + file\n","\n","        txt_file = open(filepath_txt, \"r\")\n","        content = txt_file.read()\n","        content = content.split(\"\\n\")\n","        txt_file.close()\n","\n","        for line in content:\n","            if len(line) > 0:\n","                id = -1\n","                num = 0\n","                while num < 2:\n","                    id += 1\n","                    if line[id] == '-':\n","                        num += 1\n","                key = line[:id+5]\n","                txt = line[id+6:]\n","                dic_flac_txt[key][1] = txt\n","\n","        #print(len(dic_flac_txt))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQrRkoS7i3Mc"},"source":["import pickle\n","\n","data_audio = []\n","data_txt = []\n","\n","for key in dic_flac_txt:\n","    data_audio.append(dic_flac_txt[key][0])\n","    data_txt.append(dic_flac_txt[key][1])\n","\n","save_filepath_data_audio = '/content/drive/My Drive/data_audio.pkl'\n","save_filepath_data_txt = '/content/drive/My Drive/data_txt.pkl'\n","\n","with open(save_filepath_data_audio, 'wb') as f:\n","    pickle.dump(data_audio, f)\n","\n","with open(save_filepath_data_txt, 'wb') as f:\n","    pickle.dump(data_txt, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qO0IsrdUi3DZ"},"source":["import pickle\n","\n","save_filepath_data_audio = '/content/drive/My Drive/data_audio.pkl'\n","save_filepath_data_txt = '/content/drive/My Drive/data_txt.pkl'\n","\n","with open(save_filepath_data_audio, 'rb') as f:\n","    data_audio = pickle.load(f)\n","\n","with open(save_filepath_data_txt, 'rb') as f:\n","    data_txt = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOEaFLxqxBh6"},"source":["import pickle\n","import torch\n","\n","data_audio_embedding = []\n","\n","for i in range(len(data_audio)):\n","    #print(i)\n","    with torch.no_grad():\n","        input_values = tokenizer(data_audio[i], return_tensors=\"pt\").input_values\n","        hidden_states = model(input_values).last_hidden_state\n","        data_audio_embedding.append(hidden_states)\n","\n","save_filepath_data_audio_embedding = '/content/drive/My Drive/data_audio_embedding.pkl'\n","\n","with open(save_filepath_data_audio_embedding, 'wb') as f:\n","    pickle.dump(data_audio_embedding, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iPwpKicxBe0"},"source":["import torch\n","import pickle\n","\n","save_filepath_data_audio_embedding = '/content/drive/My Drive/data_audio_embedding.pkl'\n","\n","with open(save_filepath_data_audio_embedding, 'rb') as f:\n","    data_audio_embedding = pickle.load(f)\n","\n","\"\"\"\n","print(\"data_audio_embedding loaded\")\n","\n","max_len = 0\n","for e in data_audio_embedding_padding_right:\n","    length = e.shape[1]\n","    if length > max_len:\n","        max_len = length\n","\n","for i in range(len(data_audio_embedding_padding_right)):\n","    length = data_audio_embedding_padding_right[i].shape[1]\n","    if length < max_len:\n","        data_audio_embedding_padding_right[i] = torch.cat((data_audio_embedding_padding_right[i], torch.zeros((1,max_len-length,768))), dim=1)\n","data_audio_embedding_padding_right = torch.cat(data_audio_embedding_padding_right, dim=0)\n","\n","print(\"data_audio_embedding_padding_right done\")\n","\n","save_filepath_data_audio_embedding_padding_right = '/content/drive/My Drive/data_audio_embedding_padding_right.pkl'\n","\n","with open(save_filepath_data_audio_embedding_padding_right, 'wb') as f:\n","    pickle.dump(data_audio_embedding_padding_right, f)\n","\n","print(\"data_audio_embedding_padding_right saved\")\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-i_FoS_0xBUE"},"source":["!mkdir '/content/drive/My Drive/data_libri_en'\n","!mkdir '/content/drive/My Drive/data_libri_en/audio'\n","!mkdir '/content/drive/My Drive/data_libri_en/labels'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0JRVb7NlDkP"},"source":["for i in range(len(data_audio_embedding)):\n","    torch.save(data_audio_embedding[i], '/content/drive/My Drive/data_libri_en/audio/audio_'+str(i)+'.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxgfA45i7Mns"},"source":["import pickle\n","\n","save_filepath_data_txt = '/content/drive/My Drive/data_txt.pkl'\n","\n","with open(save_filepath_data_txt, 'rb') as f:\n","    data_txt = pickle.load(f)\n","\n","data_txt = data_txt[:2607] # There was a None at position 2607\n","txt = '\\n'.join(data_txt)\n","text_file = open('/content/drive/My Drive/data_txt.txt', \"w\")\n","n = text_file.write(txt)\n","text_file.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9V-FE_FIlDhr"},"source":["!sudo apt-get install festival espeak-ng mbrola"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wxPQIqulDfD"},"source":["!pip install phonemizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwO3EWGMlDcE"},"source":["!phonemize -b espeak -l en-us -p '-' -w ' ' '/content/drive/My Drive/data_txt.txt' -o '/content/drive/My Drive/data_phones.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDQ4odNSlDYf"},"source":["filepath_txt = '/content/drive/My Drive/data_phones.txt'\n","txt_file = open(filepath_txt, \"r\")\n","content = txt_file.read()\n","content = content.split(\"\\n\")\n","del content[-1]\n","txt_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcYNDyiwlDUK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616312028237,"user_tz":-60,"elapsed":497,"user":{"displayName":"Hugo Laurençon","photoUrl":"","userId":"11375379862591739933"}},"outputId":"5fe6bdd2-546f-4d1a-853c-dae0fc228028"},"source":["vocab = {}\n","for i in range(len(content)):\n","    cont = content[i][:-2].replace(\" \", \"\")\n","    cont = cont.replace(\"--\", \"-\")\n","    split = cont.split('-')\n","    for e in split:\n","        if e != '':\n","            if e in vocab:\n","                vocab[e] += 1\n","            else:\n","                vocab[e] = 1\n","\n","print(len(vocab.keys()))\n","print(vocab.keys())\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["60\n","dict_keys(['ð', 'iː', 'z', 'eɪ', 's', 'p', 'ɹ', 'ɛ', 'd', 'aʊ', 't', 'ɔ', 'n', 'ə', 'ɡ', 'j', 'uː', 'ɪ', 'ŋ', 'b', 'æ', 'k', 'oʊ', 'f', 'ɔːɹ', 'l', 'w', 'v', 'ɑː', 'ɐ', 'h', 'ɑːɹ', 'aɪ', 'ᵻ', 'oːɹ', 'i', 'm', 'ʌ', 'əl', 'ɚ', 'ʊɹ', 'ʊ', 'dʒ', 'ɜː', 'ɛɹ', 'ɾ', 'tʃ', 'ɔɪ', 'ɔː', 'ʃ', 'aɪɚ', 'oː', 'θ', 'ɪɹ', 'iə', 'aɪə', 'ʒ', 'ʔ', 'n̩', 'r'])\n","{'ð': 6365, 'iː': 4280, 'z': 5349, 'eɪ': 2877, 's': 8852, 'p': 3510, 'ɹ': 6296, 'ɛ': 5148, 'd': 9478, 'aʊ': 1301, 't': 11823, 'ɔ': 761, 'n': 13098, 'ə': 7086, 'ɡ': 1713, 'j': 1161, 'uː': 3327, 'ɪ': 11005, 'ŋ': 2304, 'b': 3341, 'æ': 6497, 'k': 4862, 'oʊ': 2460, 'f': 3510, 'ɔːɹ': 813, 'l': 6423, 'w': 4409, 'v': 3940, 'ɑː': 1616, 'ɐ': 2338, 'h': 4066, 'ɑːɹ': 673, 'aɪ': 3736, 'ᵻ': 1683, 'oːɹ': 462, 'i': 2378, 'm': 5555, 'ʌ': 5539, 'əl': 1167, 'ɚ': 3165, 'ʊɹ': 187, 'ʊ': 1006, 'dʒ': 814, 'ɜː': 1585, 'ɛɹ': 491, 'ɾ': 1261, 'tʃ': 1098, 'ɔɪ': 207, 'ɔː': 1069, 'ʃ': 1422, 'aɪɚ': 127, 'oː': 122, 'θ': 863, 'ɪɹ': 266, 'iə': 230, 'aɪə': 71, 'ʒ': 90, 'ʔ': 59, 'n̩': 60, 'r': 3}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liBnAcYtnXFr","executionInfo":{"status":"ok","timestamp":1616312411689,"user_tz":-60,"elapsed":466,"user":{"displayName":"Hugo Laurençon","photoUrl":"","userId":"11375379862591739933"}},"outputId":"b58ba1b5-ef02-4fdf-d8a0-f65d4c8ca748"},"source":["map_ipa_idx = {}\n","ipa_vocab = list(vocab.keys())\n","for i in range(len(ipa_vocab)):\n","    map_ipa_idx[ipa_vocab[i]] = i+1 # the index 0 is for the blank\n","print(map_ipa_idx)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'ð': 1, 'iː': 2, 'z': 3, 'eɪ': 4, 's': 5, 'p': 6, 'ɹ': 7, 'ɛ': 8, 'd': 9, 'aʊ': 10, 't': 11, 'ɔ': 12, 'n': 13, 'ə': 14, 'ɡ': 15, 'j': 16, 'uː': 17, 'ɪ': 18, 'ŋ': 19, 'b': 20, 'æ': 21, 'k': 22, 'oʊ': 23, 'f': 24, 'ɔːɹ': 25, 'l': 26, 'w': 27, 'v': 28, 'ɑː': 29, 'ɐ': 30, 'h': 31, 'ɑːɹ': 32, 'aɪ': 33, 'ᵻ': 34, 'oːɹ': 35, 'i': 36, 'm': 37, 'ʌ': 38, 'əl': 39, 'ɚ': 40, 'ʊɹ': 41, 'ʊ': 42, 'dʒ': 43, 'ɜː': 44, 'ɛɹ': 45, 'ɾ': 46, 'tʃ': 47, 'ɔɪ': 48, 'ɔː': 49, 'ʃ': 50, 'aɪɚ': 51, 'oː': 52, 'θ': 53, 'ɪɹ': 54, 'iə': 55, 'aɪə': 56, 'ʒ': 57, 'ʔ': 58, 'n̩': 59, 'r': 60}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1yyrL4OJnqs2"},"source":["import torch\n","\n","for i in range(len(content)):\n","    cont = content[i][:-2].replace(\" \", \"\")\n","    cont = cont.replace(\"--\", \"-\")\n","    split = cont.split('-')\n","    val = []\n","    for e in split:\n","        if e != '':\n","            val.append(map_ipa_idx[e])\n","    tens = torch.tensor(val)\n","    torch.save(tens, '/content/drive/My Drive/data_libri_en/labels/label_'+str(i)+'.pt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KMqcRoEzEEW"},"source":["# Creation of the model for phone recognition"]},{"cell_type":"code","metadata":{"id":"8fYKX3gCy2RP"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","class CustomDataset:\n","    def __init__(self, dataset_path, len_dataset, train_size, batch_size):\n","        self.dataset_path = dataset_path\n","        self.len_dataset = len_dataset\n","        self.train_size = train_size\n","        self.batch_size = batch_size\n","        self.it_train = 0\n","        self.it_eval = 0\n","\n","    def get_next_train_batch(self):\n","        list_tens_audio, list_tens_labels = None, None\n","        if (self.it_train + 1) * self.batch_size <= self.train_size:\n","            list_tens_audio = [torch.load(self.dataset_path + '/audio/audio_' + str(i) + '.pt') for i in range(self.it_train * self.batch_size, (self.it_train + 1) * self.batch_size)]\n","            list_tens_labels = [torch.load(self.dataset_path + '/labels/label_' + str(i) + '.pt') for i in range(self.it_train * self.batch_size, (self.it_train + 1) * self.batch_size)]\n","            self.it_train += 1\n","        else:\n","            list_tens_audio = [torch.load(self.dataset_path + '/audio/audio_' + str(i) + '.pt') for i in range(self.it_train * self.batch_size, self.train_size)]\n","            list_tens_labels = [torch.load(self.dataset_path + '/labels/label_' + str(i) + '.pt') for i in range(self.it_train * self.batch_size, self.train_size)]\n","            self.it_train = 0\n","        input_lengths = torch.tensor([e.shape[1] for e in list_tens_audio])\n","        target_lengths = torch.tensor([e.shape[0] for e in list_tens_labels])\n","        targets = torch.cat(list_tens_labels)\n","        max_len = torch.max(input_lengths)\n","        for i in range(len(list_tens_audio)):\n","            length = list_tens_audio[i].shape[1]\n","            if length < max_len:\n","                list_tens_audio[i] = torch.cat((list_tens_audio[i], torch.zeros((1,max_len-length,768))), dim=1)\n","        X = torch.cat(list_tens_audio, dim=0)\n","        return X, input_lengths, targets, target_lengths\n","\n","    def get_next_eval_batch(self):\n","        list_tens_audio, list_tens_labels = None, None\n","        if self.train_size + (self.it_eval + 1) * self.batch_size <= self.len_dataset:\n","            list_tens_audio = [torch.load(self.dataset_path + '/audio/audio_' + str(i) + '.pt') for i in range(self.train_size + self.it_eval * self.batch_size, self.train_size + (self.it_eval + 1) * self.batch_size)]\n","            list_tens_labels = [torch.load(self.dataset_path + '/labels/label_' + str(i) + '.pt') for i in range(self.train_size + self.it_eval * self.batch_size, self.train_size + (self.it_eval + 1) * self.batch_size)]\n","            self.it_eval += 1\n","        else:\n","            list_tens_audio = [torch.load(self.dataset_path + '/audio/audio_' + str(i) + '.pt') for i in range(self.train_size + self.it_eval * self.batch_size, self.len_dataset)]\n","            list_tens_labels = [torch.load(self.dataset_path + '/labels/label_' + str(i) + '.pt') for i in range(self.train_size + self.it_eval * self.batch_size, self.len_dataset)]\n","            self.it_eval = 0\n","        input_lengths = torch.tensor([e.shape[1] for e in list_tens_audio])\n","        target_lengths = torch.tensor([e.shape[0] for e in list_tens_labels])\n","        targets = torch.cat(list_tens_labels)\n","        max_len = torch.max(input_lengths)\n","        for i in range(len(list_tens_audio)):\n","            length = list_tens_audio[i].shape[1]\n","            if length < max_len:\n","                list_tens_audio[i] = torch.cat((list_tens_audio[i], torch.zeros((1,max_len-length,768))), dim=1)\n","        X = torch.cat(list_tens_audio, dim=0)\n","        return X, input_lengths, targets, target_lengths\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MajlIDXCIo_5"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc = nn.Linear(768, 61)\n","\n","    def forward(self, x):\n","        x_drop = self.dropout(x)\n","        fc = self.fc(x_drop)\n","        output = F.log_softmax(fc, dim=2)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CIalcBA7ubm"},"source":["def train(model, device, dataset, n_epochs, learning_rate):\n","    ctc_loss = nn.CTCLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    for ep in range(n_epochs):\n","        print(\"Epoch:\", ep)\n","        model.train()\n","        for it in range(dataset.train_size // dataset.batch_size + 1*(dataset.train_size % dataset.batch_size > 0)):\n","            X, input_lengths, targets, target_lengths = dataset.get_next_train_batch()\n","            X, input_lengths, targets, target_lengths = X.to(device), input_lengths.to(device), targets.to(device), target_lengths.to(device)\n","            optimizer.zero_grad()\n","            X = model(X).permute(1,0,2)\n","            loss = ctc_loss(X, targets, input_lengths, target_lengths)\n","            loss.backward()\n","            optimizer.step()\n","            print(\"It:\", it, \"Train loss:\", loss.item())\n","        model.eval()\n","        mean_loss_eval = []\n","        with torch.no_grad():\n","            for it in range((dataset.len_dataset - dataset.train_size) // dataset.batch_size + 1*((dataset.len_dataset - dataset.train_size) % dataset.batch_size > 0)):\n","                X, input_lengths, targets, target_lengths = dataset.get_next_eval_batch()\n","                X, input_lengths, targets, target_lengths = X.to(device), input_lengths.to(device), targets.to(device), target_lengths.to(device)\n","                X = model(X).permute(1,0,2)\n","                loss = ctc_loss(X, targets, input_lengths, target_lengths)\n","                mean_loss_eval.append(loss.item())\n","        print(\"Average eval loss:\", sum(mean_loss_eval)/len(mean_loss_eval))\n","        print(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5nK1_N_npqh"},"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","dataset = CustomDataset('/content/drive/My Drive/data_libri_en', 2607, 2200, 64)\n","n_epochs = 1\n","learning_rate = 0.001\n","\n","train(model, device, dataset, n_epochs, learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iw11YQCnlDJf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616368971342,"user_tz":-60,"elapsed":8589,"user":{"displayName":"Hugo Laurençon","photoUrl":"","userId":"11375379862591739933"}},"outputId":"8f017232-b5a4-48f5-d372-d34e39214b02"},"source":["!pip install jiwer"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting jiwer\n","  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n","Collecting python-Levenshtein\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n","\r\u001b[K     |██████▌                         | 10kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 33.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30kB 36.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40kB 39.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer) (54.1.2)\n","Building wheels for collected packages: python-Levenshtein\n","  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149817 sha256=24dbda969ff052221000f90f492092c5748da5f5c48a22241bae431f72f1652a\n","  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n","Successfully built python-Levenshtein\n","Installing collected packages: python-Levenshtein, jiwer\n","Successfully installed jiwer-2.2.0 python-Levenshtein-0.12.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NaVTDfyRIo5K"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qDM0aDNGoDT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgKAKYHpGn9w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pG3p9BmzcfPz"},"source":["# Pipeline pour les expériences à réaliser\n","\n","**En priorité**\n","\n","-Choisir un modèle wav2vec (essentiellement sa taille et le/les langages sur lesquels il a été entraîné). Les différents modèles sont présents [ici](https://huggingface.co/models?filter=wav2vec2).\n","\n","-Choisir un target language pour la tâche de phone/phoneme recognition\n","\n","-Télécharger les données en format wav pour le target language, avec les labels sous forme de phrase\n","\n","-Préprocesser les données, c'est-à-dire transformer tous les wav en une liste de liste de nombre réels, que l'on va passer au modèle pré-entraîné Wav2vec pour créer les embeddings\n","\n","-Préprocesser les labels, c'est-à-dire utiliser un phonemizer (par exemple https://github.com/bootphon/phonemizer) pour transformer les phrases en une liste de phones/phonemes\n","\n","-Créer un data loader, et faire du padding sur les données audio et les labels de façon à pouvoir ensuite faire des batchs, et avoir une entrée et une sortie propre à une application d'un modèle de ML\n","\n","-Créer un modèle, que ce soit un modèle linéaire (dropout + une couche linear qui est la même pour tous les timesteps), LSTM, transformer ou autre, pour partir des données des embeddings et qui doit prédire le résultat du phonemizer. Entraîner le modèle avec la loss CTC (voir https://huggingface.co/transformers/master/_modules/transformers/models/wav2vec2/modeling_wav2vec2.html#Wav2Vec2ForCTC pour la loss)\n","\n","-Evaluation et trouver une bonne métrique\n","\n","**Par la suite**\n","\n","-Répéter les opérations précédentes mais en changeant le modèle wav2vec (et potentiellement en le remplaçant par différents CPC pré-entraîné), en changeant le target language (ou en gardant le même mais avec plus ou moins de données), et en changeant le modèle final utilisé, de façon à répondre à un maximum de questions du proposal\n","\n","**Si ça ne marche pas**\n","\n","-Mettre en plus un phone qui encode le \"silence\" entre deux mots\n","\n","-Utiliser un tokenizer non deprecated.\n","\n","-Corriger le warning du début \"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized\", qui apparemment n'est pas si grave.\n","\n"]}]}